{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DAE datos IRIS.ipynb","provenance":[{"file_id":"1gEcMqfeYOJnjxHE-AACF0t8SJTXIsif6","timestamp":1580735169391}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"r6X3sZdb3I7T","colab_type":"text"},"source":["# Demo de Deep Autoencoder (DAE) para procesar los datos de tipos de Iris\n","Basado en: \n","\n","https://blog.keras.io/building-autoencoders-in-keras.html\n","\n","\n","https://towardsdatascience.com/deep-autoencoders-using-tensorflow-c68f075fd1a3"]},{"cell_type":"markdown","metadata":{"id":"ZQ3UDqAY4zlD","colab_type":"text"},"source":["1) Importar librerías:"]},{"cell_type":"code","metadata":{"id":"NafdiEwq3IMh","colab_type":"code","colab":{}},"source":["import keras\n","import tensorflow as tf\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from keras.utils import plot_model\n","\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","\n","print(\"\\nLibrerías importadas\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eWIi5mD5rUUH","colab_type":"text"},"source":["2) Configurar los datos cargados (se debe definir de acuerdo a los datos):\n"]},{"cell_type":"code","metadata":{"id":"KIyhmAT5rUnA","colab_type":"code","colab":{}},"source":["# define atributos y clases\n","CSV_COLUMN_NAMES = ['LargoSepalo', 'AnchoSepalo', 'LargoPetalo', 'AnchoPetalo', 'Clase']\n","CLASSES = ['Setosa', 'Versicolor', 'Virginica']\n","\n","ClassAttributeName = 'Clase'\n","\n","print(\"Configuración definida.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCIbu73rrarn","colab_type":"text"},"source":["3) Cargar CSV con datos a procesar y preparar datos para entrenar y probar (ya separados):"]},{"cell_type":"code","metadata":{"id":"JbuV1Qtxrc6c","colab_type":"code","colab":{}},"source":["# levanta los datos de entrenamiento y prueba\n","train_path = tf.keras.utils.get_file(\n","    \"iris_training.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\")\n","test_path = tf.keras.utils.get_file(\n","    \"iris_test.csv\", \"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\")\n","\n","train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n","test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n","\n","# ver datos\n","train.head()\n","#test.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ay7hoSLCrgFo","colab_type":"code","colab":{}},"source":["# define datos de entrada y salida para entrenamiento\n","x_train = np.array(train.drop([ClassAttributeName], axis=1))\n","y_train = np.array(train[ClassAttributeName])\n","\n","# define datos de entrada y salida para testing\n","x_test = np.array(test.drop([ClassAttributeName], axis=1))\n","y_test = np.array(test[ClassAttributeName])\n","\n","print(\"\\n\\nDatos Originales \", len(x_train)+len(x_test))\n","print(\"- Datos para Entrenar \", len(x_train))\n","print(\"- Datos para Probar \", len(x_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqJT-899FP7H","colab_type":"text"},"source":["4) Definir la configuración del modelo DAE:"]},{"cell_type":"code","metadata":{"id":"i2uoSF7XFQuS","colab_type":"code","colab":{}},"source":["# define tamaño de datos de entrada y salida\n","num_inputs = len(x_train[0])\n","num_outputs = num_inputs\n","\n","# cantidad de neuronas ocultas para features (datos comprimidos o codings)\n","num_features = 2\n","\n","# cantidad de neuronas ocultas para la parte Encoder \n","#   (cada elemento de la lista es la cantidad de pesos que tiene cada una)\n","dae_layers = [ 3 ] \n","\n","#  agrega la capa de features a las capas\n","dae_layers.append( num_features ) \n","\n","# cantidad de neuronas ocultas para la parte Decoder \n","#   (usa la la lista de Encoder inversa)\n","for eachEncLayer in dae_layers[0:len(dae_layers)-1][::-1]:\n","  dae_layers.append( eachEncLayer )\n","\n","# cantidad de épocas del entrenamiento\n","# (a medida que la cantidad de capas ocultas se mayor y el nro de features es menor, \n","#   se recomienda entrenar más épocas)\n","cantEpocas = 500\n","\n","print(\"Configuración del DAE definida: [\", num_inputs, dae_layers, num_outputs, \"] \")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zOQAmtPAaqZg","colab_type":"text"},"source":["5) Creación del modelo DAE:"]},{"cell_type":"code","metadata":{"id":"LEwtvKsnJLBN","colab_type":"code","colab":{}},"source":["# define la arquitectura de capas del Deep Autoencoder\n","# teniendo en cuenta la definición dada anteriomente\n","input_data_Lay = Input(shape=(num_inputs,), name='input_data') # capa de entrada\n","eachLay = input_data_Lay\n","auxName = 'enc_'\n","auxId = 1 \n","for num_hid in dae_layers:  \n","\n","    # define el nombre de la capa oculta\n","    if num_features==num_hid:\n","        auxlayerName = 'features'\n","        auxName = 'dec_'\n","        auxId = auxId - 1\n","    else:\n","        auxlayerName = auxName+str(auxId)\n","        if auxName == 'enc_':\n","          auxId = auxId + 1\n","        else:\n","          auxId = auxId - 1\n","\n","    # agrega la capa oculta\n","    eachLay = Dense(num_hid, activation='relu', name=auxlayerName)(eachLay) # capas ocultas\n","\n","    if  auxlayerName == 'features':\n","      features_Lay = eachLay\n","\n","output_data_Lay = Dense(num_outputs, activation=None, name='output_data')(eachLay) # capa de salida\n","\n","# genera el modelo Deep Autoencoder\n","DAEmodel = Model(input_data_Lay, output_data_Lay, name='DAE')\n","#DAEmodel.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n","DAEmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","\n","print(\"Modelo DAE creado con \", len(DAEmodel.layers), \" capas:\")\n","DAEmodel.summary()\n","print(\"\\n\")\n","plot_model(DAEmodel, show_layer_names=True, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nn7ynSDx0nLt","colab_type":"text"},"source":["5) Entrenar el modelo DAE:"]},{"cell_type":"code","metadata":{"id":"jmBH11GuJhk1","colab_type":"code","colab":{}},"source":["# lleva a cabo el entrenamiento\n","# usando los mismos datos como entrada y salida\n","x_train_prep  = np.array(x_train).reshape(len(x_train), num_inputs)\n","DAEmodel.fit(x_train_prep, x_train_prep,\n","                epochs = cantEpocas, \n","                batch_size = 15)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8RhmI3R31Kzu","colab_type":"text"},"source":["7) Evaluar el modelo DAE entrenado solicitando que reconstruya los datos de entrenamiento:"]},{"cell_type":"code","metadata":{"id":"wfRff6b-LOxG","colab_type":"code","colab":{}},"source":["# evalua al modelo \n","resEval = DAEmodel.evaluate(x_train_prep, x_train_prep)\n","print(\"\\n>Evaluación del Modelo: \")\n","print(\"    - Error: \", resEval[0])\n","print(\"    - Exactitud: \", resEval[1]*100)\n","print(\"\\n\")\n","\n","# procesa las imágenes con el modelo \n","reconstr_data = DAEmodel.predict(x_train_prep)\n","\n","# muestra las 15 primeras imágenes \n","print(\">Ejemplos de Resultados: \")\n","for i in range(len(x_train))[:5]:\n","      \n","    # muestra la real\n","    print(\"  + Original: \", x_train[i])\n","    # muestra la generada por el modelo\n","    print(\"  * Reconstr: \", reconstr_data[i])\n","    print(\"  - Dif:      \", reconstr_data[i]-x_train[i])\n","    print(\"\\n\")  \n","\n","print(\"> Total de Diferencias \", sum(reconstr_data-x_train))\n","print(\"> Promedio de Diferencias \", sum(reconstr_data-x_train)/len(x_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0a79AY0r1Eo","colab_type":"text"},"source":["9) Probar el modelo DAE entrenado con los datos de test:"]},{"cell_type":"code","metadata":{"id":"jTha04q2Hlsx","colab_type":"code","colab":{}},"source":["x_test_prep  = np.array(x_test).reshape(len(x_test), num_inputs)\n","\n","# evalua al modelo \n","resEval_test = DAEmodel.evaluate(x_test_prep, x_test_prep)\n","print(\"\\n>Evaluación del Modelo: \")\n","print(\"    - Error: \", resEval_test[0])\n","print(\"    - Exactitud: \", resEval_test[1]*100)\n","print(\"\\n\")\n","\n","# procesa las imágenes con el modelo \n","reconstr_data_test = DAEmodel.predict(x_test_prep)\n","\n","# muestra las 15 primeras imágenes \n","print(\">Ejemplos de Resultados: \")\n","for i in range(len(x_test))[:5]:\n","      \n","    # muestra la real\n","    print(\"  + Original: \", x_test[i])\n","    # muestra la generada por el modelo\n","    print(\"  * Reconstr: \", reconstr_data_test[i])\n","    print(\"  - Dif:      \", reconstr_data_test[i]-x_test[i])\n","    print(\"\\n\")  \n","\n","print(\"> Total de Diferencias \", sum(reconstr_data_test-x_test))\n","print(\"> Promedio de Diferencias \", sum(reconstr_data_test-x_test)/len(x_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFl-ipxmHrok","colab_type":"text"},"source":["8) A partir del modelo DAE entrenado, generar dos sub-modelos Encoder y Decoder:"]},{"cell_type":"markdown","metadata":{"id":"eq6z-Gj5fttb","colab_type":"text"},"source":["\n","*   Generar y usar el modelo Encoder para 'clusterizar' los datos de entrenamiento:\n"]},{"cell_type":"code","metadata":{"id":"G6aAZfo2H3HW","colab_type":"code","colab":{}},"source":["## Generar el sub-modelo Encoder para Clustering\n","## (desde input hasta features)\n","\n","# reutiliza las capas entrenadas del modelo DAE original\n","clust_input_Lay = input_data_Lay  # capa de entrada\n","clust_output_Lay =  features_Lay  # capa de salida\n","\n","# genera el modelo\n","CLUSTmodel = Model(input_data_Lay, features_Lay, name='Encoder/Clustering')\n","\n","print(\"> Modelo Encoder: \")\n","CLUSTmodel.summary()\n","plot_model(CLUSTmodel, show_layer_names=True, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zGA6OetOfuX8","colab_type":"code","colab":{}},"source":[" # función auxiliar para generar un gráfico \n"," def genera_grafico(datos, clases, titulo):  \n","    principalDf = pd.DataFrame(data = datos,\n","                columns = ['a', 'b'])\n","    finalDf = pd.concat([principalDf, \n","                        pd.DataFrame(clases, columns = ['target'])], \n","                        axis = 1)\n","\n","    fig = plt.figure(figsize = (10,10))\n","    ax = fig.add_subplot(1,1,1) \n","    ax.set_title(titulo, fontsize = 20)\n","    for target in set(clases):\n","        indicesToKeep = finalDf['target'] == target\n","        ax.scatter(finalDf.loc[indicesToKeep, 'a'],\n","                  finalDf.loc[indicesToKeep, 'b'],\n","                  s = 50)\n","    ax.legend(CLASSES)\n","    ax.grid()\n","\n","\n","# función auxiliar para generar un gráfico \n","# con los valores codificados \n","# usando PCA para simplificarlos en 2 ejes\n","def genera_grafico_pca(datos, clases, titulo):\n","    pca = PCA(n_components=2)\n","    principalComponents = pca.fit_transform(datos)\n","    principalDf = pd.DataFrame(data = principalComponents,\n","                columns = ['pca_1', 'pca_2'])\n","    finalDf = pd.concat([principalDf, \n","                        pd.DataFrame(clases, columns = ['target'])], \n","                        axis = 1)\n","\n","    fig = plt.figure(figsize = (10,10))\n","    ax = fig.add_subplot(1,1,1) \n","    ax.set_xlabel('PCA1', fontsize = 15)\n","    ax.set_ylabel('PCA2', fontsize = 15)\n","    ax.set_title(titulo, fontsize = 20)\n","    for target in set(clases):\n","        indicesToKeep = finalDf['target'] == target\n","        ax.scatter(finalDf.loc[indicesToKeep, 'pca_2'],\n","                  finalDf.loc[indicesToKeep, 'pca_1'],\n","                  s = 50)\n","    ax.legend(CLASSES)\n","    ax.grid()\n","\n","# procesa las imágenes para recibir el valor codificado de cada una\n","x_train_encoded = CLUSTmodel.predict(x_train_prep)\n","\n","# muestra el gráfico con originales con PCA (para que tenga 2 dimensiones)\n","genera_grafico_pca(x_train, y_train, \"Representación de Datos Originales con PCA\")\n","\n","# muestra estadísticas de los datos codificados\n","minArClust = np.empty(num_features)\n","minArClust.fill(9999.99)\n","maxArClust = np.empty(num_features)\n","maxArClust.fill(-9999.99)\n","sumArClust = np.zeros(num_features)\n","for val in x_train_encoded:\n","  for i in range(num_features):\n","      sumArClust[i] = sumArClust[i]+val[i]\n","      if val[i]<minArClust[i]: \n","          minArClust[i] = val[i]\n","      if val[i]>maxArClust[i]: \n","          maxArClust[i] = val[i]\n","print(\"\\n\\n> Estadísticas de Clutering de Datos Originales codificado en \", num_features,\" valores: \")\n","print(\"- Mínimos:   \", minArClust)\n","print(\"- Máximos:   \", maxArClust)\n","print(\"- Totales:   \", sumArClust)\n","print(\"- Promedios: \", sumArClust/len(x_train_encoded))\n","print(\"\\n\\n\")\n","\n","# muestra el gráfico codificado (directo si  tiene 2 dimensiones, sino con PCA)\n","if num_features==2:\n","    genera_grafico(x_train_encoded, y_train, \"Representación de Clustering\")\n","else:\n","    genera_grafico_pca(x_train_encoded, y_train, \"Representación de Datos Originales con PCA\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wSW_l3cejWl","colab_type":"code","colab":{}},"source":["print(x_train_prep)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j_4UqnQtg1hP","colab_type":"text"},"source":["*   Generar y usar el modelo Decoder para generar nuevas datos similares a los entrenadas:"]},{"cell_type":"code","metadata":{"id":"PHueHIOYOaBg","colab_type":"code","colab":{}},"source":["## Generar el sub-modelo Decoder para Generator\n","## (desde features hasta output)\n","\n","# genera una copia del modelo DAE original para evitar romperlo\n","auxiCloneModel = keras.models.model_from_json(DAEmodel.to_json())\n","#auxiCloneModel.summary()\n","\n","# genera la nueva estructura del Generator\n","input_gen = Input(shape=(num_features,), name='input_gen') # nueva capa de entrada\n","auxLay_gen = input_gen\n","for pos in range(len(DAEmodel.layers)):\n","\n","  # obtiene el nombre de la capa actual\n","  auxName = DAEmodel.layers[pos].name  \n","  \n","  # sólo considera las capas luego de features (decoder y output)\n","  # para copiar los pesos del DAE original y actualizar la estrcutura\n","  if auxName.startswith('dec_') or auxName=='output_data':\n","    auxiCloneModel.layers[pos].set_weights(DAEmodel.layers[pos].get_weights()) \n","    auxLay_gen = auxiCloneModel.layers[pos](auxLay_gen) \n","\n","# crea el nuevo modelo Generator\n","GENmodel = Model(input_gen, auxLay_gen, name = 'Decoder/Generator')\n","\n","print(\"> Modelo Decoder: \")\n","GENmodel.summary()\n","plot_model(GENmodel, show_layer_names=True, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"54A8l5l9St3c","colab_type":"code","colab":{}},"source":["# ejecuta el Generator\n","#  usando valores definidos al azar como datos de entrada\n","cantGenerar = 3\n","\n","# genera los datos de entrada\n","# (como la codificación tiene varias posiciones con ceros \n","# se considera que sólo se ponen al azar entre 10% y el 70% de los valores, el resto queda en cero,\n","# --sino se podría hacer con \" np.random.rand(cantGenerar, num_features) \"-- )\n","arX = []\n","minRnd = 1\n","maxRnd = num_features\n","for i in range(cantGenerar):\n","  X = np.zeros(num_features)\n","  for j in range(np.random.randint(low=minRnd, high=maxRnd)):\n","      pos = np.random.randint(low=0, high=num_features-1)\n","      X[pos] = np.random.normal()\n","      \n","  arX.append( X )\n","\n","# ejecuta el modelo Generator\n","reconstr_data_gen = GENmodel.predict( np.array(arX).reshape((len(arX), num_features)) )  \n"," \n","# muestra las imágenes generadas\n","print(\"\\n> Resultados: \")\n","# muestra las 15 primeras imágenes \n","print(\"\\n>Ejemplos de Resultados: \")\n","for i in range(len(arX)):\n","      \n","    # muestra la real\n","    print(\"  + Randoms: \", arX[i])\n","    # muestra la generada por el modelo\n","    print(\"  * Constr.: \", reconstr_data_gen[i])\n","    print(\"\\n\")  "],"execution_count":null,"outputs":[]}]}
